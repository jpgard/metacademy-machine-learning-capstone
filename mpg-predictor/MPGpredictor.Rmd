---
title: 'Metacademy Machine Learning Capstone 0: MPG Prediction'
author: "Joshua Gardner"
date: "May 17, 2016"
output: html_document
---

```{r}
library(plyr)
library(tidyr)
```

#Preprocessing

The task here simply asks developers to predict the car MPG using nine attributes in the original dataset shown below:

    1. mpg:           continuous
    2. cylinders:     multi-valued discrete
    3. displacement:  continuous
    4. horsepower:    continuous
    5. weight:        continuous
    6. acceleration:  continuous
    7. model year:    multi-valued discrete
    8. origin:        multi-valued discrete
    9. car name:      string (unique for each instance)

A selection of the dataset is shown below:

  mpg cylinders displacement horsepower weight acceleration model_year origin                  car_name
1  18         8          307      130.0   3504         12.0         70      1 chevrolet chevelle malibu
2  15         8          350      165.0   3693         11.5         70      1         buick skylark 320
3  18         8          318      150.0   3436         11.0         70      1        plymouth satellite
4  16         8          304      150.0   3433         12.0         70      1             amc rebel sst
5  17         8          302      140.0   3449         10.5         70      1               ford torino
6  15         8          429      198.0   4341         10.0         70      1          ford galaxie 500

To preprocess the data, I read it in, converted cylinders to a factor variable. Additionally, because the car_name variable actually contains two data ponts--the manufacturer and the model--I split that variable into two columns, *make* and *model*, using the **separate()** function from Hadley Wickham's tidyr package. This addition makes a substantial difference for models below (for example, ir resulted in a 10% increase in the R-squared, or percentage of variance explained, in the linear model below).
    
```{r, fig.width=12}
mpgdata = read.table("auto-mpg.data", col.names = c("mpg", "cylinders", "displacement", "horsepower", "weight", "acceleration", "model_year", "origin", "car_name"), na.strings = "?")

mpgdata <- mpgdata %>%
    separate(car_name, into = c("make", "model"), sep = " ", extra = "merge") %>%
    mutate(cylinders = as.factor(cylinders), make = as.factor(make), model = as.factor(model), model_year = as.numeric(model_year), origin = factor(origin)) %>%
    na.omit()

#plain linear regression

lm.out = lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration + model_year + origin + make, data = mpgdata)
summary(lm.out)
par(mfrow = c(1,3))
plot(mpgdata$weight, mpgdata$mpg, col = mpgdata$cylinders, xlab = "weight", ylab = "mpg")
plot(mpgdata$model_year, mpgdata$mpg, col = mpgdata$cylinders, xlab = "weight", ylab = "")
plot(mpgdata$horsepower, mpgdata$mpg, col = mpgdata$cylinders, xlab = "weight", ylab = "")
sum((lm.out$resid-mpgdata$mpg)^2)/nrow(mpgdata)
```

Even from the output shown above, we can see that plain OLS regression does a fairly decent good job of modeling the training data, and even a slightly nonlinear function (such as a quadratic function) would appear to capture much of the little variation not already modeled by linear regression that includes cylinders as factors (this is to say nothing of how it would predict on other datasets, though--especially given the level of siphistication of modern cars). OLS regression achieves an R-squared of 87% on the training data (OLS regression does not provide a direct estimate of its performance on unseen data besides the standard errors, which the algorithm is designed to minimize). This is likely due to the minor variation between makes and models in the years represented in the dataset (1970-1982), which were produced in a time when the automobile market was far more uniform than today, meaning that raw physical laws--those linking fueld consumption directly to weight and power--were the most important factors in cars' fuel efficiency.

The linear regression achieves a root mean squared error, or RMSE, of 601.



```{r}
#knn regression

#lasso
library(glmnet)
#use model.matrix to transform qualitative variables into dummy variables
x = model.matrix(mpg ~ ., data = mpgdata )[,-1]
y = mpgdata$mpg
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
set.seed(2016)
train=sample(1:nrow(x), nrow(x)/2)
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
#ridge regression

#best subset selection
```


I also wanted to implement k-fold cross-validation for the linear model using the **cv.lm()** function in the DAAG package, but the "make" variable made this approach useless--8 makes had only a single observation in the dataset, and several others had very few, which meant the randomization used to generate the data partitions for cross-validation inevitably produced groups with 'make' values not seen in the training data (and therefore impossible to generate predictions for), or trained on only one observation of a given make (a sample size which would likely produce very high prediction error for those classes).

