---
title: 'Metacademy Machine Learning Capstone 0: MPG Prediction'
author: "Joshua Gardner"
date: "May 17, 2016"
output: html_document
---

```{r}
library(plyr)
library(tidyr)
```

#Preprocessing

The task here simply asks developers to predict the car MPG using nine attributes in the original dataset shown below:

    1. mpg:           continuous
    2. cylinders:     multi-valued discrete
    3. displacement:  continuous
    4. horsepower:    continuous
    5. weight:        continuous
    6. acceleration:  continuous
    7. model year:    multi-valued discrete
    8. origin:        multi-valued discrete
    9. car name:      string (unique for each instance)

A selection of the dataset is shown below:

  mpg cylinders displacement horsepower weight acceleration model_year origin                  car_name
1  18         8          307      130.0   3504         12.0         70      1 chevrolet chevelle malibu
2  15         8          350      165.0   3693         11.5         70      1         buick skylark 320
3  18         8          318      150.0   3436         11.0         70      1        plymouth satellite
4  16         8          304      150.0   3433         12.0         70      1             amc rebel sst
5  17         8          302      140.0   3449         10.5         70      1               ford torino
6  15         8          429      198.0   4341         10.0         70      1          ford galaxie 500

To preprocess the data, I read it in, converted cylinders to a factor variable. Additionally, because the car_name variable actually contains two data ponts--the manufacturer and the model--I split that variable into two columns, *make* and *model*, using the **separate()** function from Hadley Wickham's tidyr package. This addition makes a substantial difference for models below (for example, ir resulted in a 10% increase in the R-squared, or percentage of variance explained, in the linear model below).
    
```{r, fig.width=12}
mpgdata = read.table("auto-mpg.data", col.names = c("mpg", "cylinders", "displacement", "horsepower", "weight", "acceleration", "model_year", "origin", "car_name"), na.strings = "?")

mpgdata <- mpgdata %>%
    separate(car_name, into = c("make", "model"), sep = " ", extra = "merge") %>%
    mutate(cylinders = as.factor(cylinders), make = as.factor(make), model = as.factor(model), model_year = as.numeric(model_year), origin = factor(origin)) %>%
    na.omit()

#plain linear regression

lm.out = lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration + model_year + origin + make, data = mpgdata)
summary(lm.out)
hist(mpgdata$mpg)
par(mfrow = c(1,3))
plot(mpgdata$weight, mpgdata$mpg, col = mpgdata$cylinders, xlab = "weight", ylab = "mpg")
plot(mpgdata$model_year, mpgdata$mpg, col = mpgdata$cylinders, xlab = "weight", ylab = "")
plot(mpgdata$horsepower, mpgdata$mpg, col = mpgdata$cylinders, xlab = "weight", ylab = "")
sqrt(mean((lm.out$resid)^2))
```

Even from the output shown above, we can see that plain OLS regression does a fairly decent good job of modeling the training data, and even a slightly nonlinear function (such as a quadratic function) would appear to capture much of the little variation not already modeled by linear regression that includes cylinders as factors (this is to say nothing of how it would predict on other datasets, though--especially given the level of siphistication of modern cars). OLS regression achieves an R-squared of 87% on the training data (OLS regression does not provide a direct estimate of its performance on unseen data besides the standard errors, which the algorithm is designed to minimize). This is likely due to the minor variation between makes and models in the years represented in the dataset (1970-1982), which were produced in a time when the automobile market was far more uniform than today, meaning that raw physical laws--those linking fueld consumption directly to weight and power--were the most important factors in cars' fuel efficiency.

The linear regression achieves a root mean squared error, or RMSE, of around 2.8. This is quite good, especially considering that the standard deviation of the mpg in the dataset is 7.8. Let's see if another method can do better, though--if a simple method like regression can do this well, perhaps another method can capture more of the complexities in the data. However, the small sample size (n = 390) may make more complex and flexible methods difficult to train, and unreliable for prediction--we'll see how a few perform below.



```{r}
#knn regression

#ridge
library(glmnet)
#use model.matrix to transform qualitative variables into dummy variables
x = model.matrix(mpg ~ ., data = mpgdata )[,-1]
y = mpgdata$mpg
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
set.seed(2016)
train=sample(1:nrow(x), nrow(x)/2)
set.seed(1)
ridge.cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(ridge.cv.out, main = "Ridge Regression Results on Auto Data")
```

Interestingly, ridge provides no performance improvement over simple OLS regression here, and actually seems to perform slightly worse (probably because we used cross-validation, which gives each iteration a slightly smaller sample size, which is particularly harmful because the sample was already quite small). 

The inability of ridge regression to provide a performance improvement here makes sense, though, when we consider what the method was developed to do: deal with very high numbers of inputs (i.e., performing regression on extremely "wide" datasets) by forcibly shrinking coefficients toward zero (but never allowing coefficients to reach zero). However, in this case, the initial number of input variables is already quite small, and each adds at least some marginal information to the model. The cost parameter and consequent coefficient reductions of ridge regression, then, actually exclude useful information instead of eliminating noise and overfitting in this case. The variance of the model was already quite low, and making the model simpler simply increased the bias.

The uniqueness of this case is made visual in the unique plot above. The plopt shows that every incremental increase in lambda from zero actually increases the mean squared error--which means that the higher the penalty coefficient (and the more our ridge regression shrinks coefficients), the worse the model performs. Typically, we see at least some improvement in the model from ridge regression, but the fact that the penalty term consistently degrades the model gives us strong reason to avoid using it here.


```{r}
#lasso regression

#best subset selection
```


I also wanted to implement k-fold cross-validation for the linear model using the **cv.lm()** function in the DAAG package, but the "make" variable made this approach useless--8 makes had only a single observation in the dataset, and several others had very few, which meant the randomization used to generate the data partitions for cross-validation inevitably produced groups with 'make' values not seen in the training data (and therefore impossible to generate predictions for), or trained on only one observation of a given make (a sample size which would likely produce very high prediction error for those classes).

